---
title: "p8105_hw6_yw3095"
author: "Yixuan Wang"
date: "November 27, 2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(stats)
library(purrr)
library(modelr)
library(mgcv)

set.seed(1)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
```

##Problem 1

This problem focuses on homicides in 50 large U.S. cities. 

**Upload and Clean the Data.**

```{r 1.1, message=FALSE, warning=FALSE}
raw_data = read_csv(file = "./data/homicide-data.csv") %>% 
  janitor::clean_names() 

homicide = raw_data %>%  
  mutate(city_state = paste(city, state, sep = ", "),
         resolved = as.numeric(disposition == "Closed by arrest"),
         victim_age = ifelse(victim_age == "Unknown", NA, as.integer(victim_age)),
         victim_race = ifelse(victim_race == "White", "white", "non-white"),
         victim_race = fct_relevel(victim_race, "white")) %>% 
  select(resolved, victim_age, victim_race, victim_sex, city_state) %>% 
  filter(!city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL"))
```

**Using Data from Baltimore to do a Logistic Regression.**
```{r 1.2}
bal_fit_logistic = homicide %>% 
  filter(city_state == "Baltimore, MD") %>% 
  glm(resolved ~ victim_age + victim_race + victim_sex, data = ., family = binomial()) 

save(bal_fit_logistic,file = "bal_fit_logistic.RData") 

bal_fit_logistic %>% 
  broom::tidy() %>% 
    janitor::clean_names() %>% 
    mutate(OR = exp(estimate),
           conf_low = exp(estimate - 1.96*std_error),
           conf_high = exp(estimate + 1.96*std_error)) %>% 
    select(term, OR, conf_low, conf_high) %>% 
    knitr::kable(digits = 3) 
```

**Apply This Regression Model to All Cities.**

```{r 1.3}
fit_logistic = function(x){
    homicide %>% 
    filter(city_state == x) %>% 
    glm(resolved ~ victim_age + victim_race + victim_sex, data = ., family = binomial()) %>%  
    broom::tidy() %>% 
    janitor::clean_names() %>% 
    mutate(OR = exp(estimate),
           conf_low = exp(estimate - 1.96*std_error),
           conf_high = exp(estimate + 1.96*std_error)) %>%
    filter(term == "victim_racenon-white") %>% 
    select(beta = estimate, OR, conf_low, conf_high)
}

city_model = 
  tibble(city_state = unique(homicide$city_state)) %>% 
  mutate(map(.x = unique(homicide$city_state), ~fit_logistic(.x))) %>% 
  unnest 
```


**Making a plot to show the adjusted odds ratio (and CI) for solving homicides comparing non-white victims to white victims for each city**

```{r 1.4,fig.width = 12, fig.asp = .6, out.width = "90%"}
city_plot = city_model %>% 
  ggplot(aes(x = reorder(city_state, OR), y = OR, colour = city_state)) + 
  geom_point() +
  geom_errorbar(aes(ymin = conf_low, ymax = conf_high), width = 0.8) +
  labs(
    title = "The adjusted odds ratio (and CI) for solving homicides comparing non-white victims to white victims for each city",
    x = "Location",
    y = "The adjusted odds ratio and CI",
    color = "Location"
  ) + 
  theme(legend.position = "bottom", axis.text.x = element_text(angle = 90))
city_plot
```

From the plot above, we can know that Boston, MA has the highest adjusted odds ratio for solving homicides comparing non-white victims to white victims, and Tampa, FL has the lowest odds ratio. Durham, NC has the widest confidence interval.

##Problem 2

In this problem, we tried to understand the effects of several variables on a childâ€™s birthweight.

**Uploaded and Cleaned the Data.**

```{r 2.1, message=FALSE}
birthweight = read_csv(file = "./data/birthweight.csv") %>%
  janitor::clean_names()  

birthweight = birthweight %>% 
  mutate(babysex = as.factor(babysex),
         frace = as.factor(frace),
         malform = as.factor(malform),
         mrace = as.factor(mrace))

skimr::skim(birthweight)
```

Exploring the dataset, we found that there was no missing value in the dataset. The mean value of variable parity (number of live births prior to this pregnancy) was 0.0023, which shows that there were few live births prior to this pregency. Thus, all values for varibles pnumlbw (previous number of low birth weight babies) and pnumsga (number of prior small for gestational age babies) were 0, which was resonable. 


**Build a linear regression model**
```{r 2.2}
full_model = lm(bwt ~ ., data = birthweight)

stepwise = step(full_model, direction = "both", trace = 0)
stepwise

reg_model = lm(bwt ~ babysex + delwt + gaweeks + mrace + ppwt, data = birthweight)
summary(reg_model)
```

First, I used step-wise selection as a data-driven model-building process and got a linear regression model with 11 variables. And then I did some literature view work. According to article "Prediction of Birth Weight", I chose babysex, mother's weight at delivery, gestational age, mother's race, and mother's pre-pregnanacy weight as predictors in my final linear regression model.

**Making a plot to show model residuals against fitted values**
```{r 2.3}
model_plot_df = birthweight %>% 
  add_predictions(model = reg_model) %>%
  add_residuals(model = reg_model)

ggplot(model_plot_df, aes(x = pred, y = resid)) +
    geom_point() +
    labs(
      title = "Model Residuals against Fitted Values",
      x = "Prediction",
      y = "Residual"
    )
```

From the plot above, we found that the residuals "bounce randomly" around the line residual = 0; points roughly form a "horizontal band" around the line residual = 0; there were no obvious outliers in the plot. We can assume that it met the criteria of a well regression model.


**Compared my model with two models provided using violin plot.**

```{r 2.4, warning=FALSE}
cv_bwt_df =
  crossv_mc(birthweight, 100) %>% 
  mutate(train = map(train, as_tibble),
         test = map(test, as_tibble))

cv_bwt_df_test = 
  cv_bwt_df %>% 
  mutate(reg_mod = map(train, ~lm(bwt ~ bhead + blength + mrace + delwt + gaweeks + smoken + ppbmi + babysex + parity + ppwt + fincome, data = .x)),
         mod_1 = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
         mod_2 = map(train, ~lm(bwt ~ bhead * blength * babysex, data = .x))) %>% 
  mutate(rmse_reg_mod = map2_dbl(reg_mod, test, ~rmse(model = .x, data = .y)),
         rmse_mod_1 = map2_dbl(mod_1, test, ~rmse(model = .x, data = .y)),
         rmse_mod_2 = map2_dbl(mod_2, test, ~rmse(model = .x, data = .y)))
```
```{r 2.5}
cv_bwt_df_test %>% 
  select(starts_with("rmse")) %>% 
  gather(key = model, value = rmse) %>% 
  mutate(model = str_replace(model, "rmse_", ""),
         model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin() + 
  labs(
    title = "Comparison of the Models Using Violin Plot",
    x = "Model",
    y = "RMSE")
```

From the plot above, we found that my model had the lowest RMSE which was much better than two provided models. Besides, from the plot "Model Residuals against Fitted Values", we concluded that it met the criteria of a well regression model. So my model was more appropriate than other two models.

